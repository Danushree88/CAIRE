import pandas as pd
import numpy as np
import math
import random
import pickle
import os
import sys
from collections import Counter, defaultdict
from itertools import product
from copy import deepcopy

sys.path.append(os.path.dirname(os.path.dirname(__file__)))

RNG = np.random.RandomState(42)
random.seed(42)

def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))

def accuracy_score(y_true, y_pred):
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    return (y_true == y_pred).mean()

def precision_score_manual(y_true, y_pred):
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    tp = np.sum((y_true==1) & (y_pred==1))
    fp = np.sum((y_true==0) & (y_pred==1))
    return tp / (tp + fp) if (tp + fp) > 0 else 0.0

def recall_score_manual(y_true, y_pred):
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    tp = np.sum((y_true==1) & (y_pred==1))
    fn = np.sum((y_true==1) & (y_pred==0))
    return tp / (tp + fn) if (tp + fn) > 0 else 0.0

def f1_score_manual(y_true, y_pred):
    p = precision_score_manual(y_true, y_pred)
    r = recall_score_manual(y_true, y_pred)
    return 2*p*r / (p + r) if (p + r) > 0 else 0.0

def roc_auc_score_manual(y_true, y_score):
    y_true = np.asarray(y_true)
    y_score = np.asarray(y_score)
    pos = y_true == 1
    neg = y_true == 0
    n_pos = pos.sum()
    n_neg = neg.sum()
    if n_pos == 0 or n_neg == 0:
        return 0.5
    order = np.argsort(y_score)
    ranks = np.empty_like(order, dtype=float)
    ranks[order] = np.arange(1, len(y_score) + 1)
    sum_pos_ranks = ranks[pos].sum()
    auc = (sum_pos_ranks - n_pos*(n_pos+1)/2) / (n_pos * n_neg)
    return auc

def stratified_train_test_split(X, y, test_size=0.2, random_state=42):
    np.random.seed(random_state)
    X = np.asarray(X)
    y = np.asarray(y)
    indices = np.arange(len(y))
    classes = np.unique(y)
    train_idx = []
    test_idx = []
    for c in classes:
        c_idx = indices[y == c]
        n_test = max(1, int(len(c_idx) * test_size))
        np.random.shuffle(c_idx)
        test_idx.extend(c_idx[:n_test].tolist())
        train_idx.extend(c_idx[n_test:].tolist())
    np.random.shuffle(train_idx)
    np.random.shuffle(test_idx)
    return np.array(train_idx), np.array(test_idx)

def stratified_kfold_indices(y, n_splits=5, random_state=42):
    np.random.seed(random_state)
    y = np.asarray(y)
    classes = np.unique(y)
    folds = [[] for _ in range(n_splits)]
    for c in classes:
        idx = np.where(y == c)[0].tolist()
        np.random.shuffle(idx)
        for i, ind in enumerate(idx):
            folds[i % n_splits].append(ind)
    folds = [np.array(f, dtype=int) for f in folds]
    return folds

class LogisticRegressionGD:
    def __init__(self, lr=0.1, n_iter=2000, l2=0.0, verbose=False):
        self.lr = lr
        self.n_iter = n_iter
        self.l2 = l2
        self.verbose = verbose
        self.w = None
        self.b = 0.0

    def fit(self, X, y):
        X = np.asarray(X, dtype=float)
        y = np.asarray(y, dtype=float)
        n_samples, n_features = X.shape
        self.w = np.zeros(n_features)
        self.b = 0.0
        for it in range(self.n_iter):
            z = X.dot(self.w) + self.b
            p = sigmoid(z)
            error = p - y
            grad_w = (X.T.dot(error)) / n_samples + self.l2 * self.w
            grad_b = error.mean()
            self.w -= self.lr * grad_w
            self.b -= self.lr * grad_b
            if self.verbose and it % 500 == 0:
                loss = -np.mean(y*np.log(np.clip(p,1e-12,1)) + (1-y)*np.log(np.clip(1-p,1e-12,1)))
                print(f"[LR] iter {it}/{self.n_iter}, loss={loss:.5f}")
        return self

    def predict_proba(self, X):
        X = np.asarray(X, dtype=float)
        z = X.dot(self.w) + self.b
        return sigmoid(z)

    def predict(self, X, threshold=0.5):
        return (self.predict_proba(X) >= threshold).astype(int)

class KNNClassifier:
    def __init__(self, k=5):
        self.k = k
        self.X = None
        self.y = None

    def fit(self, X, y):
        self.X = np.asarray(X, dtype=float)
        self.y = np.asarray(y, dtype=int)

    def predict_proba(self, X):
        X = np.asarray(X, dtype=float)
        probs = []
        for x in X:
            dists = np.linalg.norm(self.X - x, axis=1)
            idx = np.argsort(dists)[:self.k]
            votes = self.y[idx]
            prob = votes.mean() 
            probs.append(prob)
        return np.array(probs)

    def predict(self, X, threshold=0.5):
        return (self.predict_proba(X) >= threshold).astype(int)

class DecisionTreeClassifierManual:
    def __init__(self, max_depth=5, min_samples_split=2, max_features=None):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.max_features = max_features
        self.tree = None

    def _gini(self, y):
        if len(y) == 0:
            return 0
        p1 = np.mean(y == 1)
        p0 = 1 - p1
        return 1.0 - p0**2 - p1**2

    def _best_split(self, X, y):
        n_samples, n_features = X.shape
        if self.max_features is None:
            features = range(n_features)
        else:
            if isinstance(self.max_features, float):
                k = max(1, int(self.max_features * n_features))
            else:
                k = min(n_features, int(self.max_features))
            features = np.random.choice(n_features, k, replace=False)
        best = {"impurity": self._gini(y), "feature": None, "threshold": None, "left_idx": None, "right_idx": None}
        base_impurity = best["impurity"]
        for feat in features:
            vals = np.unique(X[:, feat])
            if len(vals) <= 1:
                continue
            thresholds = (vals[:-1] + vals[1:]) / 2.0
            for thr in thresholds:
                left_mask = X[:, feat] <= thr
                right_mask = ~left_mask
                if left_mask.sum() < self.min_samples_split or right_mask.sum() < self.min_samples_split:
                    continue
                g_left = self._gini(y[left_mask])
                g_right = self._gini(y[right_mask])
                w = (left_mask.sum() / n_samples) * g_left + (right_mask.sum() / n_samples) * g_right
                if w < best["impurity"]:
                    best = {"impurity": w, "feature": feat, "threshold": thr,
                            "left_idx": np.where(left_mask)[0], "right_idx": np.where(right_mask)[0]}
        if best["feature"] is None:
            return None
        return best

    def _build_tree(self, X, y, depth=0):
        node = {}
        num_pos = int(np.sum(y == 1))
        num_neg = int(np.sum(y == 0))
        node["n_samples"] = len(y)
        node["num_pos"] = num_pos
        node["num_neg"] = num_neg
        node["prediction"] = 1 if num_pos >= num_neg else 0
        node["proba"] = num_pos / (num_pos + num_neg) if (num_pos + num_neg) > 0 else 0.0

        if depth >= self.max_depth or len(y) < self.min_samples_split or len(np.unique(y)) == 1:
            node["is_leaf"] = True
            return node

        split = self._best_split(X, y)
        if split is None:
            node["is_leaf"] = True
            return node

        node["is_leaf"] = False
        node["feature"] = split["feature"]
        node["threshold"] = split["threshold"]
        left_idx = split["left_idx"]
        right_idx = split["right_idx"]

        node["left"] = self._build_tree(X[left_idx], y[left_idx], depth+1)
        node["right"] = self._build_tree(X[right_idx], y[right_idx], depth+1)
        return node

    def fit(self, X, y):
        X = np.asarray(X, dtype=float)
        y = np.asarray(y, dtype=int)
        self.n_features_in_ = X.shape[1]
        self.tree = self._build_tree(X, y, depth=0)
        return self

    def _predict_one_proba(self, x, node):
        if node["is_leaf"]:
            return node["proba"]
        feat = node["feature"]
        thr = node["threshold"]
        if x[feat] <= thr:
            return self._predict_one_proba(x, node["left"])
        else:
            return self._predict_one_proba(x, node["right"])

    def predict_proba(self, X):
        X = np.asarray(X, dtype=float)
        return np.array([self._predict_one_proba(x, self.tree) for x in X])

    def predict(self, X, threshold=0.5):
        return (self.predict_proba(X) >= threshold).astype(int)

class DecisionTreeRegressorManual:
    def __init__(self, max_depth=3, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None

    def _mse(self, y):
        if len(y) == 0:
            return 0
        return np.mean((y - y.mean())**2)

    def _best_split(self, X, y):
        n_samples, n_features = X.shape
        best = {"mse": self._mse(y), "feature": None, "threshold": None, "left_idx": None, "right_idx": None}
        for feat in range(n_features):
            vals = np.unique(X[:, feat])
            if len(vals) <= 1:
                continue
            thresholds = (vals[:-1] + vals[1:]) / 2.0
            for thr in thresholds:
                left_mask = X[:, feat] <= thr
                right_mask = ~left_mask
                if left_mask.sum() < self.min_samples_split or right_mask.sum() < self.min_samples_split:
                    continue
                mse_left = self._mse(y[left_mask])
                mse_right = self._mse(y[right_mask])
                weighted = (left_mask.sum()*mse_left + right_mask.sum()*mse_right) / n_samples
                if weighted < best["mse"]:
                    best = {"mse": weighted, "feature": feat, "threshold": thr,
                            "left_idx": np.where(left_mask)[0], "right_idx": np.where(right_mask)[0]}
        if best["feature"] is None:
            return None
        return best

    def _build_tree(self, X, y, depth=0):
        node = {}
        node["n_samples"] = len(y)
        node["value"] = float(np.mean(y)) if len(y) > 0 else 0.0
        if depth >= self.max_depth or len(y) < self.min_samples_split:
            node["is_leaf"] = True
            return node
        split = self._best_split(X, y)
        if split is None:
            node["is_leaf"] = True
            return node
        node["is_leaf"] = False
        node["feature"] = split["feature"]
        node["threshold"] = split["threshold"]
        left_idx = split["left_idx"]
        right_idx = split["right_idx"]
        node["left"] = self._build_tree(X[left_idx], y[left_idx], depth+1)
        node["right"] = self._build_tree(X[right_idx], y[right_idx], depth+1)
        return node

    def fit(self, X, y):
        X = np.asarray(X, dtype=float)
        y = np.asarray(y, dtype=float)
        self.tree = self._build_tree(X, y, depth=0)
        return self

    def _predict_one(self, x, node):
        if node["is_leaf"]:
            return node["value"]
        feat = node["feature"]
        thr = node["threshold"]
        if x[feat] <= thr:
            return self._predict_one(x, node["left"])
        else:
            return self._predict_one(x, node["right"])

    def predict(self, X):
        X = np.asarray(X, dtype=float)
        return np.array([self._predict_one(x, self.tree) for x in X])

class RandomForestManual:
    def __init__(self, n_estimators=10, max_depth=6, min_samples_split=2, max_features='sqrt', bootstrap=True):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.max_features = max_features
        self.bootstrap = bootstrap
        self.trees = []
        self.features_indices = []

    def _get_max_features(self, n_features):
        if self.max_features == 'sqrt':
            return max(1, int(math.sqrt(n_features)))
        elif self.max_features == 'log2':
            return max(1, int(math.log2(n_features)))
        elif isinstance(self.max_features, float):
            return max(1, int(self.max_features * n_features))
        elif isinstance(self.max_features, int):
            return min(n_features, self.max_features)
        else:
            return n_features

    def fit(self, X, y):
        X = np.asarray(X, dtype=float)
        y = np.asarray(y, dtype=int)
        n_samples, n_features = X.shape
        self.trees = []
        self.features_indices = []
        for i in range(self.n_estimators):
            if self.bootstrap:
                idxs = np.random.choice(n_samples, n_samples, replace=True)
            else:
                idxs = np.arange(n_samples)
            max_feat = self._get_max_features(n_features)
            feat_idx = np.random.choice(n_features, max_feat, replace=False)
            xt = X[idxs][:, feat_idx]
            yt = y[idxs]
            tree = DecisionTreeClassifierManual(max_depth=self.max_depth,
                                                min_samples_split=self.min_samples_split,
                                                max_features=None)
            tree.fit(xt, yt)
            self.trees.append(tree)
            self.features_indices.append(feat_idx)
        return self

    def predict_proba(self, X):
        X = np.asarray(X, dtype=float)
        probs = np.zeros(X.shape[0], dtype=float)
        for tree, feat_idx in zip(self.trees, self.features_indices):
            xt = X[:, feat_idx]
            probs += tree.predict_proba(xt)
        probs = probs / len(self.trees)
        return probs

    def predict(self, X, threshold=0.5):
        return (self.predict_proba(X) >= threshold).astype(int)

class GradientBoostingClassifierManual:
    def __init__(self, n_estimators=50, learning_rate=0.1, max_depth=3, min_samples_split=2):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.trees = []
        self.initial_prediction = 0.0

    def fit(self, X, y):
        X = np.asarray(X, dtype=float)
        y = np.asarray(y, dtype=float)
        n_samples = X.shape[0]
        p = np.clip(np.mean(y), 1e-6, 1-1e-6)
        self.F = np.log(p / (1 - p)) * np.ones(n_samples)
        self.trees = []
        for m in range(self.n_estimators):
            p = sigmoid(self.F)
            residual = y - p
            tree = DecisionTreeRegressorManual(max_depth=self.max_depth, min_samples_split=self.min_samples_split)
            tree.fit(X, residual)
            update = tree.predict(X)
            self.F += self.learning_rate * update
            self.trees.append(tree)
        return self

    def predict_proba(self, X):
        X = np.asarray(X, dtype=float)
        if not hasattr(self, 'train_baseline'):
            return 0.5 * np.ones(X.shape[0])
        F = self.train_baseline * np.ones(X.shape[0])
        for tree in self.trees:
            F += self.learning_rate * tree.predict(X)
        return sigmoid(F)

    def predict(self, X, threshold=0.5):
        proba = self.predict_proba(X)
        return (proba >= threshold).astype(int)

    def fit_with_baseline(self, X, y):
        X = np.asarray(X, dtype=float)
        y = np.asarray(y, dtype=float)
        n_samples = X.shape[0]
        p = np.clip(np.mean(y), 1e-6, 1-1e-6)
        self.train_baseline = np.log(p / (1 - p))
        self.F = self.train_baseline * np.ones(n_samples)
        self.trees = []
        for m in range(self.n_estimators):
            p = sigmoid(self.F)
            residual = y - p
            tree = DecisionTreeRegressorManual(max_depth=self.max_depth, min_samples_split=self.min_samples_split)
            tree.fit(X, residual)
            update = tree.predict(X)
            self.F += self.learning_rate * update
            self.trees.append(tree)
        return self


def evaluate_model(model, X_train, y_train, X_val, y_val):
    model.fit(X_train, y_train)
    try:
        y_score = model.predict_proba(X_val)
    except Exception:
        y_pred = model.predict(X_val)
        y_score = y_pred.astype(float)
    y_pred = (y_score >= 0.5).astype(int)
    metrics = {
        "roc_auc": roc_auc_score_manual(y_val, y_score),
        "f1": f1_score_manual(y_val, y_pred),
        "precision": precision_score_manual(y_val, y_pred),
        "recall": recall_score_manual(y_val, y_pred),
        "accuracy": accuracy_score(y_val, y_pred)
    }
    return metrics

def cross_val_score_manual(ModelClass, params, X, y, n_splits=5, random_state=42):
    folds = stratified_kfold_indices(y, n_splits=n_splits, random_state=random_state)
    metrics_list = []
    for i in range(n_splits):
        val_idx = folds[i]
        train_idx = np.hstack([folds[j] for j in range(n_splits) if j != i])
        X_tr, y_tr = X[train_idx], y[train_idx]
        X_val, y_val = X[val_idx], y[val_idx]
        model = ModelClass(**params) if isinstance(ModelClass, type) else ModelClass
        if isinstance(model, GradientBoostingClassifierManual):
            model.fit_with_baseline(X_tr, y_tr)
            y_score = model.predict_proba(X_val)
            y_pred = (y_score >= 0.5).astype(int)
            metrics = {
                "roc_auc": roc_auc_score_manual(y_val, y_score),
                "f1": f1_score_manual(y_val, y_pred)
            }
        else:
            metrics = evaluate_model(model, X_tr, y_tr, X_val, y_val)
        metrics_list.append(metrics)
    avg = {}
    for k in metrics_list[0].keys():
        avg[k] = np.mean([m[k] for m in metrics_list])
    return avg

def grid_search_manual(ModelClass, grid_params, X, y, n_splits=3, metric='roc_auc'):
    keys = list(grid_params.keys())
    all_results = []
    best = {"params": None, "score": -np.inf}
    for vals in product(*[grid_params[k] for k in keys]):
        params = dict(zip(keys, vals))
        print("Testing params:", params)
        avg_metrics = cross_val_score_manual(ModelClass, params, X, y, n_splits=n_splits)
        score = avg_metrics[metric]
        print(" -> cv avg", metric, "=", score)
        all_results.append((params, avg_metrics))
        if score > best["score"]:
            best["score"] = score
            best["params"] = params
    return best["params"], best["score"], all_results

def load_featured_data(path=None):
    if path is None:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(current_dir)) 
        path = os.path.join(project_root, "data", "cart_abandonment_featured.csv")
    
    data_dir = os.path.dirname(path)
    if data_dir and not os.path.exists(data_dir):
        os.makedirs(data_dir)
        print(f"Created directory: {data_dir}")
    
    df = pd.read_csv(path)
    X = df.drop(columns=[c for c in ["abandoned", "session_id", "user_id"] if c in df.columns], errors='ignore')
    y = df["abandoned"].values
    return X.values.astype(float), y, X.columns.tolist()

def main():
    X_all, y_all, feature_names = load_featured_data()

    print("N samples:", X_all.shape[0], "N features:", X_all.shape[1])
    train_idx, test_idx = stratified_train_test_split(X_all, y_all, test_size=0.2, random_state=42)
    X_train, y_train = X_all[train_idx], y_all[train_idx]
    X_test, y_test = X_all[test_idx], y_all[test_idx]
    print("Train size:", X_train.shape[0], "Test size:", X_test.shape[0])

    print("\n=== Baseline cross-validation (5-fold) ===")
    models_to_try = {
        "LogisticRegression": (LogisticRegressionGD, {"lr": 0.1, "n_iter": 2000, "l2": 1e-4}),
        "KNN": (KNNClassifier, {"k": 5}),
        "DecisionTree": (DecisionTreeClassifierManual, {"max_depth": 6, "min_samples_split": 5}),
        "RandomForest": (RandomForestManual, {"n_estimators": 10, "max_depth": 6, "min_samples_split": 5, "max_features": 'sqrt'})
    }
    for name, (cls, params) in models_to_try.items():
        print(f"\n{name} CV:")
        avg_metrics = cross_val_score_manual(cls, params, X_train, y_train, n_splits=5)
        print(" Avg metrics:", avg_metrics)

    print("\n=== Manual grid search: RandomForest (small grid) ===")
    rf_grid = {
        "n_estimators": [10, 20],
        "max_depth": [4, 6],
        "min_samples_split": [2, 5],
        "max_features": ['sqrt']
    }
    best_rf_params, best_rf_score, rf_results = grid_search_manual(RandomForestManual, rf_grid, X_train, y_train, n_splits=3, metric='roc_auc')
    print("Best RF params:", best_rf_params, "Score:", best_rf_score)

    print("\n=== Manual grid search: GradientBoosting (small grid) ===")
    gb_grid = {
        "n_estimators": [20, 50],
        "learning_rate": [0.05, 0.1],
        "max_depth": [2, 3],
        "min_samples_split": [2]
    }
    best_gb_params, best_gb_score, gb_results = grid_search_manual(GradientBoostingClassifierManual, gb_grid, X_train, y_train, n_splits=3, metric='roc_auc')
    print("Best GB params:", best_gb_params, "Score:", best_gb_score)

    print("\n=== Final training on full training set ===")
    if best_gb_score >= best_rf_score:
        print("Selecting Gradient Boosting as final")
        final_model = GradientBoostingClassifierManual(**best_gb_params)
        final_model.fit_with_baseline(X_train, y_train)
    else:
        print("Selecting Random Forest as final")
        final_model = RandomForestManual(**best_rf_params)
        final_model.fit(X_train, y_train)

    try:
        y_score_test = final_model.predict_proba(X_test)
    except Exception:
        y_score_test = final_model.predict(X_test).astype(float)
    y_pred_test = (y_score_test >= 0.5).astype(int)

    test_metrics = {
        "roc_auc": roc_auc_score_manual(y_test, y_score_test),
        "f1": f1_score_manual(y_test, y_pred_test),
        "precision": precision_score_manual(y_test, y_pred_test),
        "recall": recall_score_manual(y_test, y_pred_test),
        "accuracy": accuracy_score(y_test, y_pred_test)
    }
    print("\nTest set metrics:", test_metrics)

    model_path = "final_manual_model.pkl"
    with open(model_path, "wb") as f:
        pickle.dump({
            "model": final_model,
            "feature_names": feature_names
        }, f)
    print(f"Saved final model to {model_path}")

if __name__ == "__main__":
    main()